{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>A Convolutional Neural Network using tensorflow</h1>\n",
    "<p>This example also uses MNIST data to create CNN that will recognize hand-written digits.\n",
    "Our expecation is that the results will be improved by using this approach versus the simple neural network.</p>\n",
    "<p>The image data looks like this:</p>\n",
    "\n",
    "<img src=\"../img/MNIST.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import tensorflow and establish our session object\n",
    "import tensorflow as tf\n",
    "session = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now make sure we have the MNIST data\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we establish the number of pixles per image and the range of digits (0 through 9) that we expect on each image.\n",
    "\n",
    "width = height = 28\n",
    "flat = width * height # = number of pixels\n",
    "class_output = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating placeholders for input and output\n",
    "\n",
    "X  = tf.placeholder(tf.float32, shape=[None, flat])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, class_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will be responsible for generating weights having some noise to prevent the vanishing gradient problem.\n",
    "\n",
    "def weight_variable(shape):\n",
    "    weight = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a function for generating a bias variable to avoid \"dying\" of neurons\n",
    "\n",
    "def bias_variable(shape):\n",
    "    bias = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Here is the structure (layers) of our CNN:<h2>\n",
    "<ol>\n",
    "    <li>Input data</li>\n",
    "    <li>Convolution and max pooling</li>\n",
    "    <li>Convolution and max pooling</li>\n",
    "    <li>Fully connected layer</li>\n",
    "    <li>Processing - DropOut</li>\n",
    "    <li>Readout layer - Fully Connected</li>\n",
    "    <li>Output - Classified digits</li>\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function for our convolution layers\n",
    "\n",
    "def conv2d(X, W):\n",
    "    return tf.nn.conv2d(X, W, strides=[1, 1, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And a function for 2x2 max pooling\n",
    "\n",
    "def max_pool_2x2(X):\n",
    "    return tf.nn.max_pool(X, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we make our first convolution and max pooling layer\n",
    "\n",
    "# Weights and Biases\n",
    "W_conv1 = weight_variable([5, 5, 1, 32]) # Size of the filter/kernel: 5x5; Input channels: 1 (greyscale); 32 feature maps\n",
    "b_conv1 = bias_variable([32]) # need 32 biases for 32 outputs\n",
    "\n",
    "# Converting images of dataset to tensors\n",
    "X_image = tf.reshape(X, [-1, 28, 28, 1])\n",
    "\n",
    "# Convolving the weight tensor and adding bias\n",
    "convolve1 = conv2d(X_image, W_conv1) + b_conv1\n",
    "\n",
    "# Appling ReLu activation\n",
    "h_conv1 = tf.nn.relu(convolve1)\n",
    "\n",
    "# Applying max pooling\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "layer1 = h_pool1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And here we make our second convolution and max pooling layer\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "convolve2 = conv2d(layer1, W_conv2) + b_conv2\n",
    "\n",
    "h_conv2 = tf.nn.relu(convolve2)\n",
    "\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "layer2 = h_pool2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the third layer - Fully connected layer to use softmax and create probabilities in the end.\n",
    "\n",
    "# Weights and bias\n",
    "# Last layer produce a feature map of 7x7, number of feature map = 64, output to softmax = 1024\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "# Flattening second layer\n",
    "layer2_matrix = tf.reshape(layer2, [-1, 7 * 7 * 64])\n",
    "\n",
    "# Applying weights and bias\n",
    "matmul_fc1 = tf.matmul(layer2_matrix, W_fc1) + b_fc1\n",
    "\n",
    "# Applying ReLu\n",
    "h_fc1 = tf.nn.relu(matmul_fc1)\n",
    "\n",
    "layer3 = h_fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droput - regularization\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "layer3_drop = tf.nn.dropout(layer3, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final layer - Read out layer (softmax, fully connected)\n",
    "\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "matmul_fc2 = tf.matmul(layer3_drop, W_fc2) + b_fc2\n",
    "\n",
    "# Applying softmax activation\n",
    "y_conv = tf.nn.softmax(matmul_fc2)\n",
    "layer4 = y_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Remember the cross entropy function from the Simple Neural Network?\n",
    "#  Let's apply something like that to layer 4\n",
    "\n",
    "cross_entropy = # TODO:  fill in code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(layer4, 1), tf.argmax(y_, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%timeit session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the training:  Again, let's start to train the CNN using mini batches of 10 for 100 iterations.\n",
    "# Once you see those results, increase both batch size and number of interations incrementally to 50/1000 and compare results.\n",
    "ITERATIONS =  # TODO: fill in here\n",
    "BATCHSIZE =   # TODO  fill in here\n",
    "\n",
    "for _ in range(ITERATIONS):\n",
    "    batch = mnist.train.next_batch(BATCHSIZE)\n",
    "    if _ % 100 == 0:\n",
    "        train_accuracy = accuracy.eval(session=session, feed_dict={X: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print (\"Step %d : %.5f\" % (_ , float(train_accuracy)))\n",
    "    train_step.run(session=session, feed_dict={X: batch[0], y_: batch[1], keep_prob: 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"test accuracy %g\"%accuracy.eval(session=session, feed_dict={X: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
