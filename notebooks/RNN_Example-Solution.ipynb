{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>A Recurrent Neural Network example using tensorflow</h1>\n",
    "<p>This example also uses MNIST data to create the RNN that will recognize hand-written digits.\n",
    "</p>\n",
    "<p>The image data looks like this:</p>\n",
    "\n",
    "<img src=\"../img/MNIST.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Overview\n",
    "\n",
    "<img src=\"../img/RNN.png\"/>\n",
    "<p>A recurrent neural network (RNN) is a class of artificial neural network where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Unlike feedforward neural networks, RNNs can use their internal state (memory) to process sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#  First import tensorflow and the rnn package\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "# Then import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's establish our training parameters\n",
    "learning_rate = 0.001\n",
    "training_steps = 10000\n",
    "batch_size = 128\n",
    "display_step = 200\n",
    "\n",
    "# And our network parameters\n",
    "num_input = 28 # MNIST data input (img shape: 28*28)\n",
    "timesteps = 28 # timesteps\n",
    "num_hidden = 128 # hidden layer num of features\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is our basic RNN \n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish some needed variables\n",
    "logits = RNN(X, weights, biases)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 2.9015, Training Accuracy= 0.070\n",
      "Step 200, Minibatch Loss= 2.1878, Training Accuracy= 0.266\n",
      "Step 400, Minibatch Loss= 1.9873, Training Accuracy= 0.391\n",
      "Step 600, Minibatch Loss= 1.8796, Training Accuracy= 0.430\n",
      "Step 800, Minibatch Loss= 1.7261, Training Accuracy= 0.453\n",
      "Step 1000, Minibatch Loss= 1.7176, Training Accuracy= 0.414\n",
      "Step 1200, Minibatch Loss= 1.6452, Training Accuracy= 0.477\n",
      "Step 1400, Minibatch Loss= 1.5781, Training Accuracy= 0.570\n",
      "Step 1600, Minibatch Loss= 1.3760, Training Accuracy= 0.570\n",
      "Step 1800, Minibatch Loss= 1.4557, Training Accuracy= 0.523\n",
      "Step 2000, Minibatch Loss= 1.3412, Training Accuracy= 0.594\n",
      "Step 2200, Minibatch Loss= 1.3234, Training Accuracy= 0.562\n",
      "Step 2400, Minibatch Loss= 1.2639, Training Accuracy= 0.641\n",
      "Step 2600, Minibatch Loss= 1.2196, Training Accuracy= 0.625\n",
      "Step 2800, Minibatch Loss= 1.2287, Training Accuracy= 0.609\n",
      "Step 3000, Minibatch Loss= 1.1813, Training Accuracy= 0.633\n",
      "Step 3200, Minibatch Loss= 1.2933, Training Accuracy= 0.539\n",
      "Step 3400, Minibatch Loss= 1.0555, Training Accuracy= 0.672\n",
      "Step 3600, Minibatch Loss= 1.1859, Training Accuracy= 0.570\n",
      "Step 3800, Minibatch Loss= 1.0063, Training Accuracy= 0.719\n",
      "Step 4000, Minibatch Loss= 0.9479, Training Accuracy= 0.648\n",
      "Step 4200, Minibatch Loss= 0.8897, Training Accuracy= 0.742\n",
      "Step 4400, Minibatch Loss= 1.0751, Training Accuracy= 0.609\n",
      "Step 4600, Minibatch Loss= 0.8778, Training Accuracy= 0.742\n",
      "Step 4800, Minibatch Loss= 0.8224, Training Accuracy= 0.742\n",
      "Step 5000, Minibatch Loss= 0.7676, Training Accuracy= 0.727\n",
      "Step 5200, Minibatch Loss= 0.9534, Training Accuracy= 0.672\n",
      "Step 5400, Minibatch Loss= 0.8551, Training Accuracy= 0.758\n",
      "Step 5600, Minibatch Loss= 0.7792, Training Accuracy= 0.766\n",
      "Step 5800, Minibatch Loss= 0.7360, Training Accuracy= 0.758\n",
      "Step 6000, Minibatch Loss= 0.6681, Training Accuracy= 0.766\n",
      "Step 6200, Minibatch Loss= 0.8172, Training Accuracy= 0.773\n",
      "Step 6400, Minibatch Loss= 0.8530, Training Accuracy= 0.758\n",
      "Step 6600, Minibatch Loss= 0.7667, Training Accuracy= 0.750\n",
      "Step 6800, Minibatch Loss= 0.5906, Training Accuracy= 0.797\n",
      "Step 7000, Minibatch Loss= 0.5056, Training Accuracy= 0.883\n",
      "Step 7200, Minibatch Loss= 0.6121, Training Accuracy= 0.797\n",
      "Step 7400, Minibatch Loss= 0.9185, Training Accuracy= 0.719\n",
      "Step 7600, Minibatch Loss= 0.7097, Training Accuracy= 0.781\n",
      "Step 7800, Minibatch Loss= 0.5276, Training Accuracy= 0.852\n",
      "Step 8000, Minibatch Loss= 0.5703, Training Accuracy= 0.828\n",
      "Step 8200, Minibatch Loss= 0.6024, Training Accuracy= 0.789\n",
      "Step 8400, Minibatch Loss= 0.5849, Training Accuracy= 0.797\n",
      "Step 8600, Minibatch Loss= 0.5930, Training Accuracy= 0.859\n",
      "Step 8800, Minibatch Loss= 0.6248, Training Accuracy= 0.797\n",
      "Step 9000, Minibatch Loss= 0.4944, Training Accuracy= 0.836\n",
      "Step 9200, Minibatch Loss= 0.3931, Training Accuracy= 0.883\n",
      "Step 9400, Minibatch Loss= 0.4154, Training Accuracy= 0.828\n",
      "Step 9600, Minibatch Loss= 0.3876, Training Accuracy= 0.906\n",
      "Step 9800, Minibatch Loss= 0.3803, Training Accuracy= 0.898\n",
      "Step 10000, Minibatch Loss= 0.2908, Training Accuracy= 0.922\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.890625\n",
      "Training took 550.8510408401489 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Let's find our start time so we can see how long the training takes\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# And now we start training\n",
    "\n",
    "with tf.Session() as session:\n",
    "\n",
    "    # Run the initializer\n",
    "    session.run(init)\n",
    "\n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "        # Run optimization op (backprop)\n",
    "        session.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = session.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        session.run(accuracy, feed_dict={X: test_data, Y: test_label}))\n",
    "    \n",
    "    print(\"Training took\", time.time()-start, \"seconds.\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
